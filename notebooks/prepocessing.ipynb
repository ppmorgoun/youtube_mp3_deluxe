{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "pd.options.display.max_rows = 70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the perfect dataset\n",
    "\n",
    "The objective goal of this model is to classify my favorites and liked playlists into song and not songs. Both these playlists are already mainly songs (I estimate 99% music for my favorites and 95% music for the liked videos)\n",
    "\n",
    "Should my training dataset also be ~95% music and only 5% other videos? I think so as this is similar to the distribution of the target I truly care about.\n",
    "\n",
    "Metric of choice: F1 score\n",
    "\n",
    "### Type of music videos in my playlists:\n",
    "* Individual songs (with and without music videos)\n",
    "* Mixes\n",
    "\n",
    "### Types of 'other' videos found in my playlists:\n",
    "* Gaming videos\n",
    "* Podcasts\n",
    "* Video essays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "songsPath = '../data/music_data/'\n",
    "nonSongsPath = '../data/non_music_data'\n",
    "\n",
    "# Populate a dataframe each for all songs and non-songs\n",
    "\n",
    "# Songs first\n",
    "songsCSVList = Path(songsPath).rglob('*.csv')\n",
    "music_df = pd.concat([pd.read_csv(str(i), index_col = 0) for i in songsCSVList])\n",
    "\n",
    "# Now non-songs\n",
    "nonSongsCSVList = Path(nonSongsPath).rglob('*.csv')\n",
    "nonMusic_df = pd.concat([pd.read_csv(str(i), index_col = 0) for i in nonSongsCSVList])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desired number of non_songs = 44 relative to 828 songs\n",
      "The shape of the full dataframe is (872, 5) where 95.0% of the observations are songs and 5.0% are not\n"
     ]
    }
   ],
   "source": [
    "# Adding a target variable column where music = 1, not music = 0\n",
    "\n",
    "music_df['is_song'] = 1\n",
    "nonMusic_df['is_song'] = 0\n",
    "\n",
    "# Joining them into one dataset\n",
    "# Got to balance the dataset first to ensure no more than 5% of the video are non-songs\n",
    "\n",
    "num_songs = len(music_df)\n",
    "num_nonSongs = len(nonMusic_df)\n",
    "percent_songs = 0.95 # desired class balance in terms of percentage of songs\n",
    "desired_num = round((num_songs / percent_songs) - num_songs)\n",
    "# Shuffle non_music df\n",
    "balancedNonMusic_df = nonMusic_df.sample(frac=1).reset_index(drop=True)\n",
    "balancedNonMusic_df = balancedNonMusic_df[:desired_num]\n",
    "\n",
    "print(f\"Desired number of non_songs = {desired_num} relative to {num_songs} songs\")\n",
    "\n",
    "\n",
    "df = pd.concat([music_df, balancedNonMusic_df], axis=0).reset_index().drop(labels='index', axis=1)\n",
    "is_song_pct, is_not_song_pct = df['is_song'].value_counts(normalize=True)\n",
    "print(f'The shape of the full dataframe is {df.shape} where {round(is_song_pct, 2)*100}% of the observations are songs and {round(is_not_song_pct, 2)*100}% are not')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MVP:\n",
    "## Random forest model baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>URL</th>\n",
       "      <th>Index</th>\n",
       "      <th>Duration</th>\n",
       "      <th>is_song</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Title, URL, Index, Duration, is_song]\n",
       "Index: []"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Having a look at the NaN rows and detemining I can probably drop them\n",
    "df.loc[[i for i in df.index if df.loc[i, :].isna().sum() != 0], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 0 observations (old shape = (872, 5)), new shape = (872, 5)\n"
     ]
    }
   ],
   "source": [
    "# Dropping NaN rows\n",
    "old_n = df.shape[0]\n",
    "df = df.dropna(axis=0)\n",
    "print(f'Dropped {old_n - df.shape[0]} observations (old shape = {(old_n, df.shape[1])}), new shape = {df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Title', 'URL', 'Index', 'Duration', 'is_song'], dtype='object')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((592, 2), (592,), (149, 2), (149,), (131, 2), (131,))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline RF model\n",
    "# Imports \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score as roc_auc\n",
    "from sklearn.metrics import accuracy_score as acc\n",
    "\n",
    "# Splitting the data\n",
    "\n",
    "features = ['Title', 'Duration']\n",
    "\n",
    "target = 'is_song'\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.2)\n",
    "indexes = {'train': X_train.index.tolist(), 'val': X_val.index.tolist(), 'test': X_test.index.tolist()}\n",
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Encoding features\n",
    "oe = OrdinalEncoder()\n",
    "X_train_matrix = oe.fit_transform(X_train)\n",
    "X_test_matrix = oe.fit_transform(X_test)\n",
    "X_val_matrix = oe.fit_transform(X_val)\n",
    "X_train = pd.DataFrame(X_train_matrix, columns=X_train.columns)\n",
    "X_test = pd.DataFrame(X_test_matrix, columns=X_test.columns)\n",
    "X_val = pd.DataFrame(X_val_matrix, columns=X_val.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('simple_imputer', SimpleImputer()),\n",
       "                ('classifier', RandomForestClassifier(n_jobs=-1))])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "simple_pipeline = Pipeline(steps=[\n",
    "    ('simple_imputer', SimpleImputer(strategy='mean')),\n",
    "    ('classifier', RandomForestClassifier(n_jobs = -1))\n",
    "    ]\n",
    ")\n",
    "\n",
    "simple_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The ROC AUC score of my baseline model is: 1.0\n",
      "The validation ROC AUC score of my baseline model is: 0.5284172661870504\n",
      "\n",
      "The accuracy score of my baseline model is: 1.0\n",
      "The validation accuracy score of my baseline model is: 0.8993288590604027\n"
     ]
    }
   ],
   "source": [
    "# Results: \n",
    "\n",
    "y_pred = simple_pipeline.predict(X_val)\n",
    "y_pred_train = simple_pipeline.predict(X_train)\n",
    "\n",
    "roc_auc_val = roc_auc(y_val, y_pred)\n",
    "roc_auc_train = roc_auc(y_train, y_pred_train)\n",
    "\n",
    "print(f'The ROC AUC score of my baseline model is: {roc_auc_train}')\n",
    "print(f'The validation ROC AUC score of my baseline model is: {roc_auc_val}')\n",
    "\n",
    "# Calculating accuracy\n",
    "acc_val = acc(y_val, y_pred)\n",
    "acc_train = acc(y_train, y_pred_train)\n",
    "\n",
    "print(f'\\nThe accuracy score of my baseline model is: {acc_train}')\n",
    "print(f'The validation accuracy score of my baseline model is: {acc_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of observations that were misclassified: 11\n",
      "93.0% accuracy\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title_x</th>\n",
       "      <th>Duration</th>\n",
       "      <th>is_song_pred</th>\n",
       "      <th>is_song</th>\n",
       "      <th>Title_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>844</th>\n",
       "      <td>130.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>The Social Media Version Of Your Ex-Girlfriend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>105.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Rays of Hope</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>141.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Venom - About as Bad as You Expect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>124.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>The (Only) Problem With Thor Ragnarok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>870</th>\n",
       "      <td>117.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Star Wars Cantina Band Auditions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>14.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>BROODS - Bridges</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2FeetBino- Naked [Official Audio]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>142.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>What It’s Like To Be Super High (POV)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>50.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Font Conference</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>122.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>TRAVELING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>145.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Young Thug - Hot ft. Gunna (Official Audio)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Title_x  Duration  is_song_pred  is_song  \\\n",
       "844    130.0      12.0             1        0   \n",
       "442    105.0       2.0             0        1   \n",
       "840    141.0     101.0             1        0   \n",
       "831    124.0      97.0             1        0   \n",
       "870    117.0      86.0             1        0   \n",
       "90      14.0      27.0             0        1   \n",
       "466      3.0       0.0             0        1   \n",
       "855    142.0      30.0             1        0   \n",
       "862     50.0      22.0             1        0   \n",
       "349    122.0       3.0             0        1   \n",
       "570    145.0      28.0             0        1   \n",
       "\n",
       "                                            Title_y  \n",
       "844  The Social Media Version Of Your Ex-Girlfriend  \n",
       "442                                    Rays of Hope  \n",
       "840              Venom - About as Bad as You Expect  \n",
       "831           The (Only) Problem With Thor Ragnarok  \n",
       "870                Star Wars Cantina Band Auditions  \n",
       "90                                 BROODS - Bridges  \n",
       "466               2FeetBino- Naked [Official Audio]  \n",
       "855           What It’s Like To Be Super High (POV)  \n",
       "862                                 Font Conference  \n",
       "349                                       TRAVELING  \n",
       "570     Young Thug - Hot ft. Gunna (Official Audio)  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's have a look at which validation observations the model got incorrect\n",
    "X_val_check = X_val.set_index([indexes['val']])\n",
    "X_val_check['is_song_pred'] = y_pred\n",
    "X_val_check['is_song'] = y_val\n",
    "X_val_check = X_val_check[X_val_check['is_song_pred'] != X_val_check['is_song']]\n",
    "print(f'The number of observations that were misclassified: {X_val_check.shape[0]}\\n'\n",
    "    f'{(1-round(X_val_check.shape[0] /  X_val.shape[0], 2))*100}% accuracy')\n",
    "\n",
    "titles = df[['Title']]\n",
    "to_inspect = X_val_check.merge(titles, how='inner', left_index=True, right_index=True)\n",
    "to_inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'eli5'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-92-103956c14007>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Data leakage? Let's check out the permutation scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# using eli5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0meli5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0meli5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPermutationImportance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'eli5'"
     ]
    }
   ],
   "source": [
    "# Data leakage? Let's check out the permutation scores\n",
    "# using eli5\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "\n",
    "# Defining our main preprocessing function:\n",
    "def preprocess(X):\n",
    "    X = X.copy()\n",
    "    \n",
    "    # Changing all the NaN values to zeros\n",
    "    X = X.fillna(0)\n",
    "\n",
    "    oe = OrdinalEncoder()\n",
    "    X = oe.fit_transform(X)\n",
    "\n",
    "\n",
    "    return X\n",
    "\n",
    "X_train_temp = preprocess(X_train)\n",
    "X_val_temp = preprocess(X_val)\n",
    "\n",
    "model = RandomForestClassifier(n_jobs=-1)\n",
    "\n",
    "model.fit(X_train_temp, y_train)\n",
    "\n",
    "permuter = PermutationImportance(\n",
    "    model,\n",
    "    n_iter = 5\n",
    ")\n",
    "permuter.fit(X_val_temp, y_val)\n",
    "\n",
    "# Viewing the top permutation importance features\n",
    "feature_names = X_train.columns.to_list()\n",
    "\n",
    "eli5.show_weights(\n",
    "    permuter,\n",
    "    top=10,\n",
    "    feature_names = feature_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categories 0.255 +/- 0.013\n",
      "duration 0.067 +/- 0.007\n",
      "upload_date 0.025 +/- 0.005\n"
     ]
    }
   ],
   "source": [
    "### Me trying to check permutation importances using sklearn's library\n",
    "\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "r = permutation_importance(simple_pipeline, X_val, y_val, n_repeats=40)\n",
    "for i in r.importances_mean.argsort()[::-1]:\n",
    "     if r.importances_mean[i] - 2 * r.importances_std[i] > 0:\n",
    "         print(f\"{X.columns[i]} \"\n",
    "               f\"{r.importances_mean[i]:.3f}\"\n",
    "               f\" +/- {r.importances_std[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results of my baseline model:\n",
    "it looks like I'm achieving a perfect score on these data. Could it be that this data is easily distinguishable between\n",
    "songs and non-songs, do I have data leakage going on, or is my data unrealistically simplified?\n",
    "\n",
    "Reasons why this might be:\n",
    "* My non-music videos consists of tiny channels (Mia Maria, Payo, the sweedish investor) which might be giving my model a simple way to determine which videos are not music by this metric\n",
    "* this is evident in the permutation importance scores as all three 'view_count', 'like_count', 'dislike_count' for in the top 3\n",
    "\n",
    "## Things to try a bit differently:\n",
    "* Remove mia maria's videos and get some non-music videos in there that have similar view counts to the music videos\n",
    "\n",
    "## Results after axing 'view_count', 'like_count', 'dislike_count':\n",
    "* F1 score for the validation set has dropped to 0.996\n",
    "* Permutation score is now showing the most important feature is average rating of 0.0717, followed by categories at about half that (0.0409)\n",
    "\n",
    "Maybe this shows the model is actually surprisingly great for this (relatively) simple task?\n",
    "\n",
    "\n",
    "# Things I could try next:\n",
    "* Oversampling the non-songs in my dataset to make it more balanced, and then measuring accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_jobs=-1)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distributions = { \n",
    "    'classifier__n_estimators': randint(50, 500), \n",
    "    'classifier__max_depth': [5, 10, 15, 20, None], \n",
    "    'classifier__max_features': uniform(0, 1), \n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    simple_pipeline, \n",
    "    param_distributions=param_distributions, \n",
    "    n_iter=10, \n",
    "    cv=2, \n",
    "    scoring='f1', \n",
    "    verbose=10, \n",
    "    return_train_score=True, \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "search.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters {'classifier__max_depth': 15, 'classifier__max_features': 0.7708555135882567, 'classifier__n_estimators': 163}\n",
      "Cross-validation F1 score 0.9910689543811397\n"
     ]
    }
   ],
   "source": [
    "print('Best hyperparameters', search.best_params_)\n",
    "print('Cross-validation F1 score', search.best_score_)\n",
    "tuned_model = search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1 score of my tuned model is: 1.0\n",
      "The validation F1 score of my tuned model is: 0.9511201629327903\n"
     ]
    }
   ],
   "source": [
    "# Double checking my F1 scores\n",
    "# Results: \n",
    "\n",
    "y_pred = tuned_model.predict(X_val)\n",
    "y_pred_train = tuned_model.predict(X_train)\n",
    "\n",
    "f1_val = f1(y_val, y_pred)\n",
    "f1_train = f1(y_train, y_pred_train)\n",
    "\n",
    "print(f'The F1 score of my tuned model is: {f1_train}')\n",
    "print(f'The validation F1 score of my tuned model is: {f1_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('3.8.3': pyenv)",
   "language": "python",
   "name": "python38364bit383pyenv9fc8690d20b94dae95512165dd4116a0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
